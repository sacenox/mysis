# Ollama providers (local)
[providers.ollama-qwen]
endpoint = "http://localhost:11434"
model = "qwen3:8b"
temperature = 0.7

[providers.ollama-qwen-small]
endpoint = "http://localhost:11434"
model = "qwen3:4b"
temperature = 0.7

[providers.ollama-llama]
endpoint = "http://localhost:11434"
model = "llama3.1:8b"
temperature = 0.7

# OpenCode Zen providers (cloud)
[providers.zen-nano]
endpoint = "https://opencode.ai/zen/v1"
model = "gpt-5-nano"
api_key_name = "opencode_zen"
temperature = 0.7

[providers.zen-pickle]
endpoint = "https://opencode.ai/zen/v1"
model = "big-pickle"
api_key_name = "opencode_zen"
temperature = 0.7

[mcp]
upstream = "https://game.spacemolt.com/mcp"
upstream_version = "v0.43.0"
